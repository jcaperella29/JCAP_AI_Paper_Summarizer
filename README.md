# ğŸ§  JCAP AI Paper Summarizer

An AI-powered web application that summarizes research papers using [Ollama](https://ollama.com/) and local LLMs like **Mistral**.

Supports:
- âœ… Single PDF summarization
- âœ… Batch ZIP upload of multiple PDFs
- âœ… Parallelized chunk processing
- âœ… Web-based interface with velvet-blue theming

> This is a frontend interface only. It requires a running Ollama backend.

---

## ğŸš€ Features

| Feature              | Description                                 |
|----------------------|---------------------------------------------|
| ğŸ” PDF Parsing        | Extracts and chunks full paper text         |
| ğŸ§  LLM Summarization  | Sends chunked text to Ollama (Mistral)      |
| ğŸ“ ZIP Upload         | Summarize multiple PDFs at once             |
| âš¡ Parallel Processing | Threads summarization for speed            |
| ğŸ’¾ Download           | Save summary as `.txt`                      |
| ğŸŒˆ Themed UI          | Inspired by Velvet Room / Persona aesthetic |

---

---

## ğŸ–¼ï¸ Screenshots

### ğŸ§  Main Interface

Upload a single PDF or a batch of papers as a `.zip`:
<p align="center">
  <img src="screenshots/main_interface.png" alt="Main interface" width="700">
</p>

---

### ğŸ“„ Batch Summary Results

Each PDF is summarized in its own tab with section breakdowns:
<p align="center">
  <img src="screenshots/results.png" alt="Summary results with tabs" width="700">
</p>




## ğŸ“¦ Project Structure

JCAP_AI_Paper_Summarizer/ â”œâ”€â”€ app.py # Flask app â”œâ”€â”€ requirements.txt â”œâ”€â”€ Dockerfile # Dockerfile for frontend (Flask) â”œâ”€â”€ ollama-backend/
â”‚ â””â”€â”€ Dockerfile # Dockerfile to host Ollama server â”œâ”€â”€ templates/ # HTML templates â”œâ”€â”€ static/ # Custom CSS styles â”‚ â””â”€â”€ style.css




---

## ğŸ“‚ Requirements

- Python 3.11+
- [Ollama](https://ollama.com/) running and serving models
- Models: `mistral`, `llama3`, etc.

---

## ğŸ”§ Local Dev Setup

### 1. Create virtual environment

in bash run 
python3 -m venv venv
source venv/bin/activate

2. Install dependencies

pip install -r requirements.txt

3. Start Flask server
python3 app.py
App runs at:
http://localhost:8080

â˜ï¸ Cloud Deployment
âœ… Frontend: Flask via Cloud Run
Make sure Ollama is running and publicly accessible.

Build and deploy:

gcloud run deploy jcap-summarizer \
  --source . \
  --platform managed \
  --region us-central1 \
  --allow-unauthenticated

Cloud Run will build from the included Dockerfile.
ğŸ§  Ollama Backend (Required)
This app requires an external Ollama backend running at:


http://<YOUR_OLLAMA_HOST>:11434/api/generate
âœ³ï¸ Option A: Run Locally
curl -fsSL https://ollama.com/install.sh | sh
ollama serve
ollama pull mistral
âœ³ï¸ Option B: Run in Docker (Remote or Local)

cd ollama-backend
docker build -t ollama-server .
docker run -d -p 11434:11434 ollama-server
Ollama must be running before deploying the Flask app.

ğŸ”’ Production Notes
Ollama backend must be secured if deployed publicly (add HTTPS/auth/reverse proxy).

Cloud Run app expects Ollama at a static IP or domain.

For professional use, we recommend deploying Ollama on:

A GCE VM

A private VPS

Your own server

ğŸ“„ License
MIT



